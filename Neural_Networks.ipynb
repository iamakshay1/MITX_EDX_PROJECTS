{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook for Neural Nets Week ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for ReLU Activation Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit(x):\n",
    "    \"\"\" Returns the ReLU of x, or the maximum between 0 and x.\"\"\"\n",
    "    # TODO\n",
    "    a=np.array([0,x])\n",
    "    return np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Derivative of RELU Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit_derivative(x):\n",
    "    \"\"\" Returns the derivative of ReLU.\"\"\"\n",
    "    # TODO\n",
    "    a=np.max(np.array([0,x]))\n",
    "    if a>0:\n",
    "        return np.int(1)\n",
    "    else:\n",
    "        return np.int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Training Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x1, x2, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "        input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "        hidden_layer_weighted_input = self.input_to_hidden_weights.dot(input_values) + self.biases #(3 by 1 matrix)\n",
    "        ReLU_vec = np.vectorize(rectified_linear_unit)  # Vectorize ReLU function\n",
    "        hidden_layer_activation = ReLU_vec(hidden_layer_weighted_input) #(3 by 1 matrix)\n",
    "\n",
    "        output = self.hidden_to_output_weights.dot(hidden_layer_activation)\n",
    "        activated_output = output_layer_activation(output)\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "        output_layer_error = -(y - activated_output) #dC/df(u1)\n",
    "        \n",
    "        output_derivative_vec = np.vectorize(output_layer_activation_derivative)    # Vectorize derivative of output activation\n",
    "        hidden_layer_error = np.multiply(output_derivative_vec(activated_output),self.hidden_to_output_weights.transpose())*output_layer_error #(3 by 1 matrix)\n",
    "        \n",
    "        ReLU_derivative_vec = np.vectorize(rectified_linear_unit_derivative) # Vectorize ReLU derivative\n",
    "        bias_gradients = np.multiply(hidden_layer_error, ReLU_derivative_vec(hidden_layer_weighted_input)) #dC/db\n",
    "\n",
    "        hidden_to_output_weight_gradients = np.multiply(hidden_layer_activation, output_layer_error).transpose() #dC/dV\n",
    "        input_to_hidden_weight_gradients = bias_gradients.dot(input_values.transpose()) #dC/dW\n",
    "\n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "        self.biases = self.biases - self.learning_rate*bias_gradients\n",
    "        self.input_to_hidden_weights = self.input_to_hidden_weights - self.learning_rate*input_to_hidden_weight_gradients\n",
    "        self.hidden_to_output_weights = self.hidden_to_output_weights - self.learning_rate*hidden_to_output_weight_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict Function for Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x1, x2):\n",
    "\n",
    "    input_values = np.matrix([[x1],[x2]])\n",
    "\n",
    "    # Compute output for a single input(should be same as the forward propagation in training)\n",
    "    hidden_layer_weighted_input = self.input_to_hidden_weights.dot(input_values) + self.biases\n",
    "    ReLU_vec = np.vectorize(rectified_linear_unit)\n",
    "    hidden_layer_activation = ReLU_vec(hidden_layer_weighted_input)\n",
    "    output = self.hidden_to_output_weights.dot(hidden_layer_activation)\n",
    "    activated_output = output_layer_activation(output)\n",
    "\n",
    "    return activated_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    model = nn.Sequential(\n",
    "              nn.Conv2d(1, 32, (3, 3)),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d((2, 2)),\n",
    "              nn.Conv2d(32, 64, (3,3)),\n",
    "              nn.ReLU(),\n",
    "              nn.MaxPool2d((2, 2)),\n",
    "              Flatten(),\n",
    "              nn.Linear(1600,128),\n",
    "              nn.Dropout(p = 0.5),\n",
    "              nn.Linear(128,10)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully Connected Network Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, input_dimension):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = nn.Linear(input_dimension, 64)\n",
    "        self.linear2 = nn.Linear(64, 20)\n",
    "        \n",
    "def forward(self, x):\n",
    "        xf = self.flatten(x)\n",
    "        xl1 = self.linear1(xf)\n",
    "        xl2 = self.linear2(xl1)\n",
    "        out_first_digit = xl2[:,:10]\n",
    "        out_second_digit = xl2[:,10:]\n",
    "        \n",
    "        return out_first_digit, out_second_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, input_dimension):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(1, 32, (3, 3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool2d = nn.MaxPool2d((2,2))\n",
    "        self.conv2d_2 = nn.Conv2d(32, 64, (3, 3))\n",
    "        self.flatten = Flatten() \n",
    "        self.linear1 = nn.Linear(2880, 64)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.linear2 = nn.Linear(64, 20)\n",
    "        \n",
    "def forward(self, x):\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x) \n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        out_first_digit = x[:,:10]\n",
    "        out_second_digit = x[:,10:]\n",
    "        \n",
    "        return out_first_digit, out_second_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}