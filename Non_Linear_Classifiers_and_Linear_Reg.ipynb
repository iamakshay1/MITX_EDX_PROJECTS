{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bit87baafe13b864092a35e2a1e77786fef",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Week_2 Solutions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Closed Form Solution for Linear Regression##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    import numpy as np\n",
    "    theta=np.dot(np.linalg.inv(np.dot(np.transpose(X),X)+lambda_factor*np.identity(X.shape[1])),(np.dot(np.transpose(X),Y)))\n",
    "    return theta\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Vector Machines ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for One_Vs_Rest Svm##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_rest_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for binary classifciation\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point\n",
    "    \"\"\"\n",
    "    svm_model=LinearSVC(random_state=0,C=0.1)\n",
    "    svm_model.fit(train_x,train_y)\n",
    "    prediction=svm_model.predict(test_x)\n",
    "    return prediction\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiclass SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for multiclass classifciation using a one-vs-rest strategy\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (int) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (int) for each test data point\n",
    "    \"\"\"\n",
    "    svm_model=LinearSVC(random_state=0,C=0.1,multi_class=\"ovr\")\n",
    "    svm_model.fit(train_x,train_y)\n",
    "    prediction=svm_model.predict(test_x)\n",
    "    return prediction\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multinomial Regression (SOFTMAX REGRESSION) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Computing Probabilities ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
    "    for j = 0, 1, ..., k-1\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns:\n",
    "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    e_exp = np.matmul(theta, np.transpose(X) / temp_parameter)\n",
    "    c = np.amax(e_exp,axis=0)\n",
    "    h_num = np.exp(e_exp - c)\n",
    "    h_den = np.sum(np.exp(e_exp -c),axis=0)\n",
    "    H = np.divide(h_num, h_den)\n",
    "    return H\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Cost Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    k = theta.shape[0]\n",
    "    \n",
    "    # Get number of examples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # avg error term\n",
    "    \n",
    "    # Clip prob matrix to avoid NaN instances\n",
    "    clip_prob_matrix = np.clip(compute_probabilities(X, theta, temp_parameter), 1e-15, 1-1e-15)\n",
    "    \n",
    "    # Take the log of the matrix of probabilities\n",
    "    log_clip_matrix = np.log(clip_prob_matrix)\n",
    "    \n",
    "    # Create a sparse matrix of [[y(i) == j]]\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape = (k,n)).toarray()\n",
    "    \n",
    "    # Only add terms of log(matrix of prob) where M == 1\n",
    "    error_term = (-1/n)*np.sum(log_clip_matrix[M == 1])    \n",
    "                \n",
    "    # Regularization term\n",
    "    reg_term = (lambda_factor/2)*np.linalg.norm(theta)**2\n",
    "    \n",
    "    return error_term + reg_term\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function For Gradient Descent ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Runs one step of batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        alpha - the learning rate (scalar)\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    k = theta.shape[0]\n",
    "    \n",
    "    # Get number of examples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Create spare matrix of [[y(i) == j]]\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape=(k,n)).toarray()\n",
    "    \n",
    "    # Matrix of Probabilities\n",
    "    P = compute_probabilities(X, theta, temp_parameter)\n",
    "    \n",
    "    # Gradient matrix of theta\n",
    "\n",
    "    grad_theta = (-1/(temp_parameter*n))*(np.matmul((M-P),X)) + lambda_factor*theta\n",
    "    \n",
    "    # Gradient descent update of theta matrix\n",
    "    theta = theta - alpha*grad_theta\n",
    "    \n",
    "    return theta\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Labels Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_y(train_y, test_y):\n",
    "    \"\"\"\n",
    "    Changes the old digit labels for the training and test set for the new (mod 3)\n",
    "    labels.\n",
    "\n",
    "    Args:\n",
    "        train_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                 for each datapoint in the training set\n",
    "        test_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                for each datapoint in the test set\n",
    "\n",
    "    Returns:\n",
    "        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                     for each datapoint in the training set\n",
    "        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                    for each datapoint in the test set\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    train_y_mod3=np.remainder(train_y,3)\n",
    "    test_y_mod3=np.remainder(test_y,3)\n",
    "    return train_y_mod3,test_y_mod3\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_error_mod3(X, Y, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Returns the error of these new labels when the classifier predicts the digit. (mod 3)\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        test_error - the error rate of the classifier (scalar)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    y_pred = get_classification(X, theta, temp_parameter)\n",
    "    \n",
    "    return 1 - (np.mod(y_pred, 3) == Y).mean()\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensionality Reduction Using Principal Component Analysis ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_PC(X, pcs, n_components, feature_means):\n",
    "    \"\"\"\n",
    "    Given principal component vectors pcs = principal_components(X)\n",
    "    this function returns a new data array in which each sample in X\n",
    "    has been projected onto the first n_components principcal components.\n",
    "    \"\"\"\n",
    "    # TODO: first center data using the feature_means\n",
    "    # TODO: Return the projection of the centered dataset\n",
    "    #       on the first n_components principal components.\n",
    "    #       This should be an array with dimensions: n x n_components.\n",
    "    # Hint: these principal components = first n_components columns\n",
    "    #       of the eigenvectors returned by principal_components().\n",
    "    #       Note that each eigenvector is already be a unit-vector,\n",
    "    #       so the projection may be done using matrix multiplication.\n",
    "    import numpy as np\n",
    "    X_centered = X-feature_means\n",
    "    \n",
    "    V_n = pcs[:,0:n_components]\n",
    "    \n",
    "    projected_data = np.matmul(X_centered,V_n)\n",
    "    \n",
    "    return projected_data\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kernels Method ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Polynomial Kernel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X, Y, c, p):\n",
    "    \"\"\"\n",
    "        Compute the polynomial kernel between two matrices X and Y::\n",
    "            K(x, y) = (<x, y> + c)^p\n",
    "        for each pair of rows x in X and y in Y.\n",
    "\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            c - a coefficient to trade off high-order and low-order terms (scalar)\n",
    "            p - the degree of the polynomial kernel\n",
    "\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    kernel_matrix = (X.dot(Y.T) + c)**p\n",
    "    \n",
    "    return kernel_matrix\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gaussian RBF Kernel ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X, Y, gamma):\n",
    "    \"\"\"\n",
    "        Compute the Gaussian RBF kernel between two matrices X and Y::\n",
    "            K(x, y) = exp(-gamma ||x-y||^2)\n",
    "        for each pair of rows x in X and y in Y.\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            gamma - the gamma parameter of gaussian function (scalar)\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    # Naive version with for loops   \n",
    "#    kernel_matrix = np.zeros((n,m))\n",
    "#    \n",
    "##### Single for loop ####\n",
    "#    for i in range(n):\n",
    "#        d = X[i,:] - Y  # Using numpy broadcasting to get differences\n",
    "#        b = np.sum(d**2, axis=1)\n",
    "#        kernel_matrix[i,:] = np.exp(-gamma*b)\n",
    "#### End: single-loop version ####\n",
    "\n",
    "#### Vectorized version (runs slower than the single loop version) ####\n",
    "#    kernel_matrix = np.linalg.norm((X[:,None] - Y), ord=2, axis=2)**2\n",
    "#    kernel_matrix = (np.sum(X**2, axis=1)[:,None] + np.sum(Y**2, axis=1)) - 2*(X @ Y.T)\n",
    "    kernel_matrix = X**2 @ np.ones((d,m)) + np.ones((n,d)) @ Y.T**2 - 2*(X @ Y.T)\n",
    "    kernel_matrix = np.exp(-gamma*kernel_matrix)\n",
    "#### End: vectorized version ####\n",
    "    \n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}