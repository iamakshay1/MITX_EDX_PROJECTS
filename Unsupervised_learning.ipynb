{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collaborative Filtering ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for E-Step Implementation ##\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep(X: np.ndarray, mixture):\n",
    "    \"\"\"E-step: Softly assigns each datapoint to a gaussian component\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        mixture: the current gaussian mixture\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the assignment\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    mu, var, pi = mixture  # Unpack mixture tuple\n",
    "    K = mu.shape[0]\n",
    "    \n",
    "    # Compute normal dist. matrix: (N, K)\n",
    "    pre_exp = (2*np.pi*var)**(d/2)\n",
    "    \n",
    "    # Calc exponent term: norm matrix/(2*variance)\n",
    "    post = np.linalg.norm(X[:,None] - mu, ord=2, axis=2)**2   # Vectorized version\n",
    "    post = np.exp(-post/(2*var))\n",
    "    \n",
    "#    post = np.zeros((n, K), dtype=np.float64) # For loop version: Array to hold posterior probs and normal matrix\n",
    "#    for i in range(n):  # Use single loop to complete Normal matrix: faster than broadcasting in 3D\n",
    "#        dist = X[i,:] - mu     # Compute difference: will be (K,d) for each n\n",
    "#        norm = np.sum(dist**2, axis=1)  # Norm: will be (K,) for each n\n",
    "#        post[i,:] = np.exp(-norm/(2*var))   # This is the exponent term of normal\n",
    "    \n",
    "    post = post/pre_exp     # Final Normal matrix: will be (n, K)\n",
    "\n",
    "    numerator = post*pi\n",
    "    denominator = np.sum(numerator, axis=1).reshape(-1,1) # This is the vector p(x;theta)\n",
    " \n",
    "    post = numerator/denominator    # This is the matrix of posterior probs p(j|i)\n",
    "    \n",
    "    log_lh = np.sum(np.log(denominator), axis=0).item()    # Log-likelihood\n",
    "    \n",
    "    return post, log_lh\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Run#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X: np.ndarray, mixture,\n",
    "        post: np.ndarray):\n",
    "    \"\"\"Runs the mixture model\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the current assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    old_log_lh = None\n",
    "    new_log_lh = None  # Keep track of log likelihood to check convergence\n",
    "    \n",
    "    # Start the main loop\n",
    "    while old_log_lh is None or (new_log_lh - old_log_lh > 1e-6*np.abs(new_log_lh)):\n",
    "        \n",
    "        old_log_lh = new_log_lh\n",
    "        \n",
    "        # E-step\n",
    "        post, new_log_lh = estep(X, mixture)\n",
    "        \n",
    "        # M-step\n",
    "        mixture = mstep(X, post)\n",
    "            \n",
    "    return mixture, post, new_log_lh\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Bayesian Information Criteria ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(X: np.ndarray, mixture,\n",
    "        log_likelihood):\n",
    "    \"\"\"Computes the Bayesian Information Criterion for a\n",
    "    mixture of gaussians\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        mixture: a mixture of spherical gaussian\n",
    "        log_likelihood: the log-likelihood of the data\n",
    "    Returns:\n",
    "        float: the BIC for this mixture\n",
    "    \"\"\"\n",
    "    # Number of examples\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Number of adjustable params = total params - 1 (last prob = 1 - sum(other probs))\n",
    "    p = 0\n",
    "    for i in range(len(mixture)):\n",
    "        if i == 0:\n",
    "            p += mixture[i].shape[0] * mixture[i].shape[1]  # For means: K times d\n",
    "        else:\n",
    "            p += mixture[i].shape[0]    # Other params: just add K\n",
    "    p = p - 1\n",
    "    \n",
    "    # BIC: log_lh - (1/2)*p*log(n)\n",
    "    bic = log_likelihood - (p*np.log(n))/2.0\n",
    "    \n",
    "    return bic\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for E-Step 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep(X: np.ndarray, mixture):\n",
    "    \"\"\"E-step: Softly assigns each datapoint to a gaussian component\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        mixture: the current gaussian mixture\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the assignment\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    mu, var, pi = mixture   # Unpack mixture tuple\n",
    "    K = mu.shape[0]\n",
    "    \n",
    "######## Loop version to calculate norms: 2nd fastest ########\n",
    "    \n",
    "    # f(u,j) matrix that's used to store the normal matrix and log of posterior probs: (p(j|u))\n",
    "#    f = np.zeros((n,K), dtype=np.float64)\n",
    "#    \n",
    "#    # Compute the normal matrix: Single loop implementation\n",
    "#    for i in range(n):\n",
    "#        # For each user pick only columns that have ratings\n",
    "#        Cu_indices = X[i,:] != 0\n",
    "#        # Dimension of Cu (no. of non-zero entries)\n",
    "#        dim = np.sum(Cu_indices)\n",
    "#        # log of pre-exponent for this user's gaussian dist.\n",
    "#        pre_exp = (-dim/2.0)*np.log((2*np.pi*var))\n",
    "#        # Calculate the exponent term of the gaussian\n",
    "#        diff = X[i, Cu_indices] - mu[:, Cu_indices]    # This will be (K,|Cu|)\n",
    "#        norm = np.sum(diff**2, axis=1)  # This will be (K,)\n",
    "#        \n",
    "#        # Now onto the final log normal matrix: log(N(...))\n",
    "#        # We will need log(normal), exp will cancel, so no need to calculate it\n",
    "#        f[i,:] = pre_exp - norm/(2*var)  # This is the ith users log gaussian dist vector: (K,)\n",
    "    \n",
    "######## End: loop version ########\n",
    "    \n",
    "######## Vectorized version to calculate norms ########\n",
    "    \n",
    "    # Create a delta matrix to indicate where X is non-zero, which will help us pick Cu indices\n",
    "    delta = X.astype(bool).astype(int)\n",
    "    # Exponent term: norm matrix/(2*variance)\n",
    "#    f = np.sum(((X[:, None, :] - mu)*delta[:, None, :])**2, axis=2)/(2*var) # This is using 3D broadcasting: slowest of all\n",
    "    f = (np.sum(X**2, axis=1)[:,None] + (delta @ mu.T**2) - 2*(X @ mu.T))/(2*var) # This is using indicator matrix: fastest of all\n",
    "    # Pre-exponent term: A matrix of shape (n, K)\n",
    "    pre_exp = (-np.sum(delta, axis=1).reshape(-1,1)/2.0) @ (np.log((2*np.pi*var)).reshape(-1,1)).T\n",
    "    # Put them together\n",
    "    f = pre_exp - f\n",
    "    \n",
    "######## End: vectorized version ########\n",
    "    \n",
    "    f = f + np.log(pi + 1e-16)  # This is the f(u,j) matrix\n",
    "    \n",
    "    # log of normalizing term in p(j|u)\n",
    "    logsums = logsumexp(f, axis=1).reshape(-1,1)  # Store this to calculate log_lh\n",
    "    log_posts = f - logsums # This is the log of posterior prob. matrix: log(p(j|u))\n",
    "    \n",
    "    log_lh = np.sum(logsums, axis=0).item()   # This is the log likelihood\n",
    "    \n",
    "    return np.exp(log_posts), log_lh\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing M-Step 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mstep(X: np.ndarray, post: np.ndarray, mixture,\n",
    "          min_variance):\n",
    "    \"\"\"M-step: Updates the gaussian mixture by maximizing the log-likelihood\n",
    "    of the weighted dataset\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        mixture: the current gaussian mixture\n",
    "        min_variance: the minimum variance for each gaussian\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    mu_rev, _, _ = mixture\n",
    "    K = mu_rev.shape[0]\n",
    "    \n",
    "    # Calculate revised pi(j): same expression as in the naive case\n",
    "    pi_rev = np.sum(post, axis=0)/n\n",
    "    \n",
    "    # Create delta matrix indicating where X is non-zero\n",
    "    delta = X.astype(bool).astype(int)\n",
    "    \n",
    "    # Update means only when sum_u(p(j|u)*delta(l,Cu)) >= 1\n",
    "    denom = post.T @ delta # Denominator (K,d): Only include dims that have information\n",
    "    numer = post.T @ X  # Numerator (K,d)\n",
    "    update_indices = np.where(denom >= 1)   # Indices for update\n",
    "    mu_rev[update_indices] = numer[update_indices]/denom[update_indices] # Only update where necessary (denom>=1)\n",
    "    \n",
    "    # Update variances\n",
    "    denom_var = np.sum(post*np.sum(delta, axis=1).reshape(-1,1), axis=0) # Shape: (K,)\n",
    "    \n",
    "######## Loop version for norms calc. ##########\n",
    "    \n",
    "    # Norm matrix for variance calc\n",
    "#    norms = np.zeros((n, K), dtype=np.float64)\n",
    "#    \n",
    "#    for i in range(n):\n",
    "#        # For each user pick only columns that have ratings\n",
    "#        Cu_indices = X[i,:] != 0\n",
    "#        diff = X[i, Cu_indices] - mu_rev[:, Cu_indices]    # This will be (K,|Cu|)\n",
    "#        norms[i,:] = np.sum(diff**2, axis=1)  # This will be (K,)\n",
    "    \n",
    "######## End: loop version #########\n",
    "        \n",
    "######## Vectorized version for norms calc. ########\n",
    "    \n",
    "#    norms = np.sum(((X[:, None, :] - mu_rev)*delta[:, None, :])**2, axis=2)\n",
    "    norms = np.sum(X**2, axis=1)[:,None] + (delta @ mu_rev.T**2) - 2*(X @ mu_rev.T)\n",
    "    \n",
    "######## End: vectorized version #########\n",
    "    \n",
    "    # Revised var: if var(j) < 0.25, set it = 0.25\n",
    "    var_rev = np.maximum(np.sum(post*norms, axis=0)/denom_var, min_variance)  \n",
    "    \n",
    "    return GaussianMixture(mu_rev, var_rev, pi_rev)\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing Run 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X: np.ndarray, mixture,\n",
    "        post: np.ndarray):\n",
    "    \"\"\"Runs the mixture model\n",
    "    Args:\n",
    "        X: (n, d) array holding the data\n",
    "        post: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "    Returns:\n",
    "        GaussianMixture: the new gaussian mixture\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the current assignment\n",
    "    \"\"\"\n",
    "    old_log_lh = None\n",
    "    new_log_lh = None  # Keep track of log likelihood to check convergence\n",
    "    \n",
    "    # Start the main loop\n",
    "    while old_log_lh is None or (new_log_lh - old_log_lh > 1e-6*np.abs(new_log_lh)):\n",
    "        \n",
    "        old_log_lh = new_log_lh\n",
    "        \n",
    "        # E-step\n",
    "        post, new_log_lh = estep(X, mixture)\n",
    "        \n",
    "        # M-step\n",
    "        mixture = mstep(X, post, mixture)\n",
    "            \n",
    "    return mixture, post, new_log_lh\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Collaborative Filtering ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estep(X: np.ndarray, mixture):\n",
    "    \"\"\"E-step: Softly assigns each datapoint to a gaussian component\n",
    "    Args:\n",
    "        X: (n, d) array holding the data, with incomplete entries (set to 0)\n",
    "        mixture: the current gaussian mixture\n",
    "    Returns:\n",
    "        np.ndarray: (n, K) array holding the soft counts\n",
    "            for all components for all examples\n",
    "        float: log-likelihood of the assignment\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    mu, var, pi = mixture   # Unpack mixture tuple\n",
    "    K = mu.shape[0]\n",
    "    \n",
    "######## Loop version to calculate norms: 2nd fastest ########\n",
    "    \n",
    "    # f(u,j) matrix that's used to store the normal matrix and log of posterior probs: (p(j|u))\n",
    "#    f = np.zeros((n,K), dtype=np.float64)\n",
    "#    \n",
    "#    # Compute the normal matrix: Single loop implementation\n",
    "#    for i in range(n):\n",
    "#        # For each user pick only columns that have ratings\n",
    "#        Cu_indices = X[i,:] != 0\n",
    "#        # Dimension of Cu (no. of non-zero entries)\n",
    "#        dim = np.sum(Cu_indices)\n",
    "#        # log of pre-exponent for this user's gaussian dist.\n",
    "#        pre_exp = (-dim/2.0)*np.log((2*np.pi*var))\n",
    "#        # Calculate the exponent term of the gaussian\n",
    "#        diff = X[i, Cu_indices] - mu[:, Cu_indices]    # This will be (K,|Cu|)\n",
    "#        norm = np.sum(diff**2, axis=1)  # This will be (K,)\n",
    "#        \n",
    "#        # Now onto the final log normal matrix: log(N(...))\n",
    "#        # We will need log(normal), exp will cancel, so no need to calculate it\n",
    "#        f[i,:] = pre_exp - norm/(2*var)  # This is the ith users log gaussian dist vector: (K,)\n",
    "    \n",
    "######## End: loop version ########\n",
    "    \n",
    "######## Vectorized version to calculate norms ########\n",
    "    \n",
    "    # Create a delta matrix to indicate where X is non-zero, which will help us pick Cu indices\n",
    "    delta = X.astype(bool).astype(int)\n",
    "    # Exponent term: norm matrix/(2*variance)\n",
    "#    f = np.sum(((X[:, None, :] - mu)*delta[:, None, :])**2, axis=2)/(2*var) # This is using 3D broadcasting: slowest of all\n",
    "    f = (np.sum(X**2, axis=1)[:,None] + (delta @ mu.T**2) - 2*(X @ mu.T))/(2*var) # This is using indicator matrix: fastest of all\n",
    "    # Pre-exponent term: A matrix of shape (n, K)\n",
    "    pre_exp = (-np.sum(delta, axis=1).reshape(-1,1)/2.0) @ (np.log((2*np.pi*var)).reshape(-1,1)).T\n",
    "    # Put them together\n",
    "    f = pre_exp - f\n",
    "    \n",
    "######## End: vectorized version ########\n",
    "    \n",
    "    f = f + np.log(pi + 1e-16)  # This is the f(u,j) matrix\n",
    "    \n",
    "    # log of normalizing term in p(j|u)\n",
    "    logsums = logsumexp(f, axis=1).reshape(-1,1)  # Store this to calculate log_lh\n",
    "    log_posts = f - logsums # This is the log of posterior prob. matrix: log(p(j|u))\n",
    "    \n",
    "    log_lh = np.sum(logsums, axis=0).item()   # This is the log likelihood\n",
    "    \n",
    "    return np.exp(log_posts), log_lh\n",
    "def fill_matrix(X: np.ndarray, mixture) -> np.ndarray:\n",
    "    \"\"\"Fills an incomplete matrix according to a mixture model\n",
    "    Args:\n",
    "        X: (n, d) array of incomplete data (incomplete entries =0)\n",
    "        mixture: a mixture of gaussians\n",
    "    Returns\n",
    "        np.ndarray: a (n, d) array with completed data\n",
    "    \"\"\"\n",
    "    X_pred = X.copy()\n",
    "    mu, _, _ = mixture\n",
    "    \n",
    "    post, _ = estep(X, mixture)\n",
    "    \n",
    "    # Missing entries to be filled\n",
    "    miss_indices = np.where(X == 0)\n",
    "    X_pred[miss_indices] = (post @ mu)[miss_indices]\n",
    "    \n",
    "    return X_pred\n",
    "def fill_matrix(X: np.ndarray, mixture) -> np.ndarray:\n",
    "    \"\"\"Fills an incomplete matrix according to a mixture model\n",
    "    Args:\n",
    "        X: (n, d) array of incomplete data (incomplete entries =0)\n",
    "        mixture: a mixture of gaussians\n",
    "    Returns\n",
    "        np.ndarray: a (n, d) array with completed data\n",
    "    \"\"\"\n",
    "    X_pred = X.copy()\n",
    "    mu, _, _ = mixture\n",
    "    \n",
    "    post, _ = estep(X, mixture)\n",
    "    \n",
    "    # Missing entries to be filled\n",
    "    miss_indices = np.where(X == 0)\n",
    "    X_pred[miss_indices] = (post @ mu)[miss_indices]\n",
    "    \n",
    "    return X_pred\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}